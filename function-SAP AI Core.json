[{"id":"0c257153-a94f-4442-a323-13a0430263d8","userId":"069f62fe-3479-4827-984a-f4d12dee91ca","function":{"id":"sap_ai_core","name":"SAP AI Core","meta":{"description":"Provides support for SAP AI Core as a Provider","type":"pipe","manifest":{"title":"SAP AI Core","author":"schardosin","date":"2025-07-07","version":"7.5 (Stable)","license":"MIT","description":"A pipe for SAP AI Core provider.","requirements":"requests"}},"content":"\"\"\"\ntitle: SAP AI Core\nauthor: schardosin\ndate: 2025-07-07\nversion: 7.5 (Stable)\nlicense: MIT\ndescription: A pipe for SAP AI Core provider.\nrequirements: requests\n\"\"\"\n\nimport os\nimport json\nimport requests\nimport ast  # Added for safely parsing non-standard JSON from Anthropic\nfrom datetime import datetime, timedelta, timezone\nfrom pydantic import BaseModel, Field\nfrom typing import List, Union, Generator, Dict\n\n# --- Main Pipe Class ---\n\n\nclass Pipe:\n    \"\"\"\n    Connects to SAP AI Core using direct REST API calls. This implementation\n    avoids the SAP GenAI Hub SDK and LangChain for greater control and stability.\n    \"\"\"\n\n    class Valves(BaseModel):\n        \"\"\"Configuration settings managed in the Open WebUI 'Pipes' panel.\"\"\"\n\n        AI_CORE_CLIENT_ID: str = Field(default=\"\", title=\"SAP AI Core Client ID\")\n        AI_CORE_CLIENT_SECRET: str = Field(\n            default=\"\", title=\"SAP AI Core Client Secret\"\n        )\n        AI_CORE_AUTH_URL: str = Field(\n            default=\"\",\n            title=\"SAP AI Core Auth URL (e.g., https://<subdomain>.authentication.sap.hana.ondemand.com)\",\n        )\n        AI_CORE_BASE_URL: str = Field(\n            default=\"\",\n            title=\"SAP AI Core Base URL (e.g., https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com)\",\n        )\n        AI_CORE_RESOURCE_GROUP: str = Field(\n            default=\"default\", title=\"SAP AI Core Resource Group\"\n        )\n        MAX_TOKENS: int = Field(default=8192, title=\"Max Tokens\")\n        TEMPERATURE: float = Field(default=0.7, title=\"Default Temperature\")\n        STREAM: bool = Field(default=True, title=\"Enable Streaming\")\n\n    def __init__(self):\n        \"\"\"Initializes the pipe state. Token and deployments are initially None.\"\"\"\n        self.valves = self.Valves()\n        self.token_info: Dict = {}\n        self.deployments: List[Dict] = []\n\n    # --- Authentication ---\n\n    def _get_token(self) -> str:\n        \"\"\"\n        Retrieves a valid OAuth token, authenticating if necessary.\n        Caches the token until it expires.\n        \"\"\"\n        # Check if token exists and is not expired (with a 60-second buffer)\n        if (\n            self.token_info\n            and self.token_info.get(\"expires_at\", 0)\n            > (datetime.now(timezone.utc) + timedelta(seconds=60)).timestamp()\n        ):\n            return self.token_info[\"access_token\"]\n\n        print(\"Authenticating with SAP AI Core...\")\n\n        token_url = f\"{self.valves.AI_CORE_AUTH_URL.rstrip('/')}/oauth/token\"\n        payload = {\n            \"grant_type\": \"client_credentials\",\n            \"client_id\": self.valves.AI_CORE_CLIENT_ID,\n            \"client_secret\": self.valves.AI_CORE_CLIENT_SECRET,\n        }\n        headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n\n        try:\n            response = requests.post(token_url, data=payload, headers=headers)\n            response.raise_for_status()  # Raises an HTTPError for bad responses (4xx or 5xx)\n\n            data = response.json()\n            expires_at = datetime.now(timezone.utc).timestamp() + data[\"expires_in\"]\n            self.token_info = {\n                \"access_token\": data[\"access_token\"],\n                \"expires_at\": expires_at,\n            }\n            print(\"✅ Authentication successful.\")\n            return self.token_info[\"access_token\"]\n        except requests.exceptions.RequestException as e:\n            print(f\"❌ Authentication failed: {e}\")\n            raise\n\n    # --- Deployment Management ---\n\n    def _get_deployments(self) -> List[Dict]:\n        \"\"\"Fetches and caches the list of running deployments from SAP AI Core.\"\"\"\n        # Return cached deployments if available\n        if self.deployments:\n            return self.deployments\n\n        print(\"Fetching deployments from SAP AI Core...\")\n        token = self._get_token()\n        headers = {\n            \"Authorization\": f\"Bearer {token}\",\n            \"AI-Resource-Group\": self.valves.AI_CORE_RESOURCE_GROUP,\n        }\n        # Using the /v2/lm/deployments endpoint as it appears to be the correct one for listing foundation model deployments.\n        url = f\"{self.valves.AI_CORE_BASE_URL.rstrip('/')}/v2/lm/deployments?$top=1000\"\n\n        try:\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n\n            api_deployments = response.json().get(\"resources\", [])\n\n            # Filter for running deployments and format them based on the /lm/deployments structure\n            formatted_deployments = []\n            for dep in api_deployments:\n                if dep.get(\"targetStatus\") == \"RUNNING\":\n                    try:\n                        # Safely navigate the nested dictionary structure from the TS example\n                        model_name = dep[\"details\"][\"resources\"][\"backend_details\"][\n                            \"model\"\n                        ][\"name\"]\n                        deployment_id = dep[\"id\"]\n                        # The internal cache stores the real deployment ID and the model name\n                        formatted_deployments.append(\n                            {\"id\": deployment_id, \"name\": model_name}\n                        )\n                    except (KeyError, TypeError):\n                        # This handles cases where the nested keys don't exist\n                        print(\n                            f\"Warning: Skipping deployment {dep.get('id')} due to unexpected data structure.\"\n                        )\n                        continue\n\n            self.deployments = formatted_deployments\n            print(f\"✅ Found {len(self.deployments)} running deployments.\")\n            return self.deployments\n        except requests.exceptions.RequestException as e:\n            print(f\"❌ Failed to fetch deployments: {e}\")\n            raise\n\n    def _get_deployment_id_for_model(self, model_name: str) -> str:\n        \"\"\"Finds the deployment ID for a given model name.\"\"\"\n        deployments = self._get_deployments()\n        for dep in deployments:\n            # Match model name exactly\n            if dep[\"name\"] == model_name:\n                return dep[\"id\"]\n        raise ValueError(f\"No running deployment found for model '{model_name}'\")\n\n    # --- Main Pipe Methods ---\n\n    def pipes(self) -> List[dict]:\n        \"\"\"\n        Called by Open WebUI to discover available models.\n        It returns a list where the model's name is used as the unique ID for the UI.\n        \"\"\"\n        if not all(\n            [\n                self.valves.AI_CORE_CLIENT_ID,\n                self.valves.AI_CORE_CLIENT_SECRET,\n                self.valves.AI_CORE_AUTH_URL,\n                self.valves.AI_CORE_BASE_URL,\n            ]\n        ):\n            return [\n                {\n                    \"id\": \"not-configured\",\n                    \"name\": \"Pipe is not configured. Please check settings.\",\n                }\n            ]\n        try:\n            # This populates self.deployments cache for internal use\n            all_deployments = self._get_deployments()\n\n            # Use a dictionary to handle cases where multiple deployments might have the same model name,\n            # ensuring the UI list has unique entries.\n            unique_models_for_ui = {\n                dep[\"name\"]: {\"id\": dep[\"name\"], \"name\": dep[\"name\"]}\n                for dep in all_deployments\n            }\n\n            # Convert the dictionary of unique models to a list\n            model_list = list(unique_models_for_ui.values())\n\n            # Sort the list alphabetically by the 'name' key\n            sorted_model_list = sorted(model_list, key=lambda x: x[\"name\"])\n\n            # Return the sorted list of unique models formatted for the UI.\n            return sorted_model_list\n        except Exception as e:\n            return [{\"id\": \"error\", \"name\": f\"Connection Error: {e}\"}]\n\n    async def pipe(self, body: dict) -> Generator[str, None, None]:\n        \"\"\"\n        Handles the inference request by constructing the correct API call\n        based on the model family.\n        \"\"\"\n        try:\n            token = self._get_token()\n            # The 'model' field from the body will now be the model name (e.g., 'sap.gpt-4o')\n            raw_model_id = body.get(\"model\", \"\")\n            # Remove the 'sap.' prefix\n            model_id = raw_model_id.split(\".\", 1)[-1]\n            # Look up the real deployment ID using the correct model name\n            deployment_id = self._get_deployment_id_for_model(model_id)\n            messages = body.get(\"messages\", [])\n\n            # --- Model-specific URL and Payload Construction ---\n\n            base_url = self.valves.AI_CORE_BASE_URL.rstrip(\"/\")\n            headers = {\n                \"Authorization\": f\"Bearer {token}\",\n                \"AI-Resource-Group\": self.valves.AI_CORE_RESOURCE_GROUP,\n                \"Content-Type\": \"application/json\",\n            }\n\n            url, payload = self._build_request_params(\n                base_url, deployment_id, model_id, messages, body\n            )\n\n            # --- Streaming API Call ---\n\n            with requests.post(\n                url, headers=headers, json=payload, stream=True\n            ) as response:\n                response.raise_for_status()\n                # Yield chunks based on the model's response format\n                for chunk in self._stream_response_parser(response, model_id):\n                    yield chunk\n\n        except Exception as e:\n            # Yield a single error message chunk\n            error_message = f\"Error during model inference: {e}\"\n            print(f\"❌ {error_message}\")\n            yield error_message\n\n    # --- Request Building and Stream Parsing ---\n\n    def _build_request_params(\n        self,\n        base_url: str,\n        deployment_id: str,\n        model_id: str,\n        messages: List[Dict],\n        body: Dict,\n    ) -> (str, Dict):\n        \"\"\"Constructs the API URL and payload based on the model ID.\"\"\"\n\n        # Convert messages to OpenAI format for simplicity, as most models use a variation of it.\n        openai_messages = [\n            {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n            for msg in messages\n            if msg.get(\"role\") != \"system\"\n        ]\n        system_prompt = next(\n            (msg[\"content\"] for msg in messages if msg.get(\"role\") == \"system\"), None\n        )\n\n        # Classify models\n        is_openai_family = any(\n            prefix in model_id for prefix in [\"gpt-\", \"o1\", \"o3\", \"o4\"]\n        )\n        is_anthropic_family = any(\n            prefix in model_id for prefix in [\"anthropic--\", \"claude\"]\n        )\n        is_gemini_family = \"gemini\" in model_id\n\n        # OpenAI-compatible models (GPT, etc.)\n        if is_openai_family:\n            url = f\"{base_url}/v2/inference/deployments/{deployment_id}/chat/completions?api-version=2024-12-01-preview\"\n            # Base payload for all OpenAI-family models\n            payload = {\n                \"messages\": (\n                    [{\"role\": \"system\", \"content\": system_prompt}]\n                    if system_prompt\n                    else []\n                )\n                + openai_messages,\n                \"stream\": True,\n            }\n            # Specific models require temperature and max_tokens to be omitted.\n            if model_id in [\"o1\", \"o3-mini\", \"o3\", \"o4-mini\"]:\n                # These parameters are intentionally omitted for these models.\n                pass\n            else:\n                # Add standard parameters for other models like gpt-4o\n                payload[\"max_tokens\"] = body.get(\"max_tokens\", self.valves.MAX_TOKENS)\n                payload[\"temperature\"] = body.get(\n                    \"temperature\", self.valves.TEMPERATURE\n                )\n\n            return url, payload\n\n        # Anthropic models (Claude)\n        if is_anthropic_family:\n            # **FIX:** Reverted to the logic that correctly identifies which models use the newer /converse-stream endpoint.\n            if any(ver in model_id for ver in [\"3.5\", \"3.7\", \"4\"]):\n                url = f\"{base_url}/v2/inference/deployments/{deployment_id}/converse-stream\"\n                payload = {\n                    \"system\": [{\"text\": system_prompt}] if system_prompt else [],\n                    \"messages\": [\n                        {\"role\": msg[\"role\"], \"content\": [{\"text\": msg[\"content\"]}]}\n                        for msg in openai_messages\n                    ],\n                    \"inference_config\": {\n                        \"max_tokens\": body.get(\"max_tokens\", self.valves.MAX_TOKENS),\n                        \"temperature\": body.get(\"temperature\", self.valves.TEMPERATURE),\n                    },\n                }\n            else:  # All other Anthropic models (including 3.0, Haiku) use the older /invoke-with-response-stream\n                url = f\"{base_url}/v2/inference/deployments/{deployment_id}/invoke-with-response-stream\"\n                payload = {\n                    \"system\": system_prompt,\n                    \"messages\": openai_messages,  # This format is correct for this endpoint\n                    \"max_tokens\": body.get(\"max_tokens\", self.valves.MAX_TOKENS),\n                    \"anthropic_version\": \"bedrock-2023-05-31\",\n                }\n            return url, payload\n\n        # Gemini models\n        if is_gemini_family:\n            url = f\"{base_url}/v2/inference/deployments/{deployment_id}/models/{model_id}:streamGenerateContent\"\n            payload = {\n                \"contents\": [\n                    {\n                        \"role\": \"user\" if msg[\"role\"] != \"assistant\" else \"model\",\n                        \"parts\": [{\"text\": msg[\"content\"]}],\n                    }\n                    for msg in openai_messages\n                ],\n                \"system_instruction\": (\n                    {\"parts\": [{\"text\": system_prompt}]} if system_prompt else None\n                ),\n                \"generation_config\": {\n                    \"max_output_tokens\": body.get(\"max_tokens\", self.valves.MAX_TOKENS),\n                    \"temperature\": body.get(\"temperature\", self.valves.TEMPERATURE),\n                },\n            }\n            return url, payload\n\n        # Default to OpenAI format if no specific family is matched\n        print(\n            f\"Warning: Model '{model_id}' not specifically classified. Defaulting to OpenAI-compatible API format.\"\n        )\n        url = f\"{base_url}/v2/inference/deployments/{deployment_id}/chat/completions?api-version=2024-12-01-preview\"\n        payload = {\n            \"messages\": (\n                [{\"role\": \"system\", \"content\": system_prompt}] if system_prompt else []\n            )\n            + openai_messages,\n            \"stream\": True,\n            \"max_tokens\": body.get(\"max_tokens\", self.valves.MAX_TOKENS),\n            \"temperature\": body.get(\"temperature\", self.valves.TEMPERATURE),\n        }\n        return url, payload\n\n    def _stream_response_parser(\n        self, response: requests.Response, model_id: str\n    ) -> Generator[str, None, None]:\n        \"\"\"\n        Parses the SSE stream from the API and yields content.\n        This version uses iter_lines for robust, line-by-line processing.\n        \"\"\"\n        for line in response.iter_lines():\n            if line:\n                decoded_line = line.decode(\"utf-8\")\n                if decoded_line.startswith(\"data: \"):\n                    json_str = decoded_line[6:]\n                    if json_str.strip() == \"[DONE]\":\n                        continue\n\n                    is_anthropic_family = any(\n                        prefix in model_id for prefix in [\"anthropic--\", \"claude\"]\n                    )\n\n                    try:\n                        # Handle Anthropic's non-standard JSON response.\n                        if is_anthropic_family:\n                            # ast.literal_eval safely parses Python literals (like dicts with single quotes)\n                            data = ast.literal_eval(json_str)\n                        else:\n                            # Other models use standard JSON\n                            data = json.loads(json_str)\n\n                        # --- Model-specific parsers ---\n                        is_openai_family = any(\n                            prefix in model_id for prefix in [\"gpt-\", \"o1\", \"o3\", \"o4\"]\n                        )\n                        is_gemini_family = \"gemini\" in model_id\n\n                        # OpenAI Parser\n                        if is_openai_family or not (\n                            is_anthropic_family or is_gemini_family\n                        ):\n                            choices = data.get(\"choices\", [])\n                            if choices:\n                                delta = choices[0].get(\"delta\", {})\n                                if \"content\" in delta and delta[\"content\"] is not None:\n                                    yield delta[\"content\"]\n\n                        # Anthropic Parser\n                        elif is_anthropic_family:\n                            # For older /invoke-with-response-stream API (Claude 3, 3.5, Haiku)\n                            if data.get(\"type\") in [\n                                \"content_block_start\",\n                                \"content_block_delta\",\n                            ]:\n                                # The actual content can be in 'content_block' or 'delta' key\n                                content_block = data.get(\n                                    \"content_block\", data.get(\"delta\", {})\n                                )\n                                if (\n                                    content_block.get(\"type\") == \"text_delta\"\n                                    or content_block.get(\"type\") == \"text\"\n                                ):\n                                    yield content_block.get(\"text\", \"\")\n                            # For newer /converse-stream API (Claude 3.7, 4)\n                            elif data.get(\"contentBlockDelta\"):\n                                delta = data[\"contentBlockDelta\"].get(\"delta\", {})\n                                if \"text\" in delta:\n                                    yield delta.get(\"text\", \"\")\n\n                        # Gemini Parser\n                        elif is_gemini_family:\n                            candidates = data.get(\"candidates\", [])\n                            if candidates and \"content\" in candidates[0]:\n                                parts = candidates[0][\"content\"].get(\"parts\", [])\n                                if parts and \"text\" in parts[0]:\n                                    yield parts[0][\"text\"]\n\n                    except (json.JSONDecodeError, ValueError, SyntaxError) as e:\n                        # Catch parsing errors from both json and ast\n                        print(\n                            f\"Warning: Could not parse data from line: {decoded_line}. Error: {e}\"\n                        )\n                        continue\n"},"info":{"body":"# SAP AI Core Function\n\nAdd SAP AI Core foundation models to Open WebUI including GPT, Claude, and Gemini with real-time streaming.\n\n## What This Does\n\nThis function connects your Open WebUI to SAP AI Core, automatically discovering your deployed models and making them available as chat options. Once configured, you'll see your SAP AI Core models in the model selector dropdown.\n\n## Setup Required\n\nConfigure these settings in the function panel:\n\n| Setting | Required | Description |\n|---------|----------|-------------|\n| **AI_CORE_CLIENT_ID** | ✅ | Client ID from your SAP AI Core service key |\n| **AI_CORE_CLIENT_SECRET** | ✅ | Client secret from your SAP AI Core service key |\n| **AI_CORE_AUTH_URL** | ✅ | Authentication URL (e.g., `https://subdomain.authentication.sap.hana.ondemand.com`) |\n| **AI_CORE_BASE_URL** | ✅ | API endpoint (e.g., `https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com`) |\n| **AI_CORE_RESOURCE_GROUP** | Optional | Resource group name (default: `default`) |\n\n## Supported Models\n\nAutomatically detects and adds your deployed models:\n- **OpenAI**: GPT-4.1, GPT-4o, GPT-4, GPT-3.5, O1/O3 series\n- **Anthropic**: Claude 3, 3.5, 3.7, 4 (all variants)\n- **Google**: Gemini Pro, Flash, and other variants\n\n## After Setup\n\n1. Save your configuration\n2. Refresh the page\n3. Your SAP AI Core models will appear in the model dropdown\n4. Select a model and start chatting\n"},"downloads":22,"upvotes":0,"downvotes":0,"updatedAt":1751983955,"createdAt":1751941205,"user":{"id":"069f62fe-3479-4827-984a-f4d12dee91ca","username":"schardosin","name":"","createdAt":1751924676,"role":null,"verified":false}}]